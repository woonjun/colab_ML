{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\82107\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\82107\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\82107\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting pytorch_lightning\n",
      "  Obtaining dependency information for pytorch_lightning from https://files.pythonhosted.org/packages/60/eb/f29a4511a2675c9c14ca31cde4562f7676cf70396cf9e599210dca2f1e66/pytorch_lightning-2.1.2-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (1.24.3)\n",
      "Collecting torch>=1.12.0 (from pytorch_lightning)\n",
      "  Obtaining dependency information for torch>=1.12.0 from https://files.pythonhosted.org/packages/d6/a8/43e5033f9b2f727c158456e0720f870030ad3685c46f41ca3ca901b54922/torch-2.1.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torch-2.1.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (2023.4.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
      "  Obtaining dependency information for torchmetrics>=0.7.0 from https://files.pythonhosted.org/packages/a3/88/cc27059747ddecff744826e38014822023cbfff4ca079a6ee9a96602dd0b/torchmetrics-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (4.7.1)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
      "  Obtaining dependency information for lightning-utilities>=0.8.0 from https://files.pythonhosted.org/packages/5e/f4/07b748cb9834848de16aaeb1ae38bc9cfcfe3adc22ee2c8ebbe11db82795/lightning_utilities-0.10.0-py3-none-any.whl.metadata\n",
      "  Downloading lightning_utilities-0.10.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\82107\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\82107\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (68.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\82107\\anaconda3\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\82107\\anaconda3\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\82107\\anaconda3\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\82107\\anaconda3\\lib\\site-packages (from tqdm>=4.57.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n",
      "Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
      "   ---------------------------------------- 0.0/776.9 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 409.6/776.9 kB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  768.0/776.9 kB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 776.9/776.9 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
      "Downloading torch-2.1.1-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/192.3 MB 17.2 MB/s eta 0:00:12\n",
      "   ---------------------------------------- 1.2/192.3 MB 19.3 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 1.5/192.3 MB 10.7 MB/s eta 0:00:18\n",
      "    --------------------------------------- 2.5/192.3 MB 13.2 MB/s eta 0:00:15\n",
      "    --------------------------------------- 3.6/192.3 MB 15.2 MB/s eta 0:00:13\n",
      "    --------------------------------------- 4.4/192.3 MB 15.7 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.2/192.3 MB 16.0 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.2/192.3 MB 16.0 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.8/192.3 MB 12.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 7.1/192.3 MB 14.2 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 8.5/192.3 MB 16.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 10.2/192.3 MB 17.2 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 11.6/192.3 MB 19.9 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 12.9/192.3 MB 22.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 14.4/192.3 MB 24.3 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 15.6/192.3 MB 32.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 17.0/192.3 MB 32.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 17.8/192.3 MB 32.7 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 19.0/192.3 MB 31.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 20.3/192.3 MB 29.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 21.5/192.3 MB 31.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 22.9/192.3 MB 29.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 24.2/192.3 MB 29.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 26.5/192.3 MB 29.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 28.1/192.3 MB 32.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 29.4/192.3 MB 32.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 30.8/192.3 MB 34.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 32.1/192.3 MB 32.7 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 32.3/192.3 MB 29.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 32.9/192.3 MB 28.4 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 33.2/192.3 MB 26.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 34.4/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 36.0/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 37.3/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 39.0/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 40.5/192.3 MB 26.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 42.2/192.3 MB 25.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 43.4/192.3 MB 31.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 45.0/192.3 MB 32.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 46.6/192.3 MB 32.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 48.5/192.3 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 49.9/192.3 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 51.9/192.3 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 53.2/192.3 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 54.7/192.3 MB 36.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 55.8/192.3 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 57.4/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 58.6/192.3 MB 32.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 60.1/192.3 MB 32.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 61.2/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 62.6/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 64.2/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 65.8/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 67.1/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 68.6/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 69.9/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 71.4/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 72.9/192.3 MB 36.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 74.3/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 75.5/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 77.1/192.3 MB 36.4 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 78.5/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 80.4/192.3 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 82.4/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 84.4/192.3 MB 36.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 86.3/192.3 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 88.2/192.3 MB 38.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 89.7/192.3 MB 38.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 90.2/192.3 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 91.1/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 92.5/192.3 MB 31.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 94.6/192.3 MB 32.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 96.3/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 97.8/192.3 MB 31.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 99.4/192.3 MB 31.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 101.2/192.3 MB 38.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 102.8/192.3 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 104.5/192.3 MB 38.5 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 105.9/192.3 MB 36.3 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 107.5/192.3 MB 34.4 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.3/192.3 MB 16.4 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 110.9/192.3 MB 16.4 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 113.2/192.3 MB 17.2 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 115.4/192.3 MB 16.8 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 117.1/192.3 MB 16.8 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 119.1/192.3 MB 17.7 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 120.8/192.3 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 122.7/192.3 MB 43.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 124.0/192.3 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 125.3/192.3 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 127.2/192.3 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 128.2/192.3 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 129.4/192.3 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 131.2/192.3 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 132.2/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 133.6/192.3 MB 32.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 134.5/192.3 MB 29.8 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 136.1/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 137.9/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 139.1/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 140.3/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 142.0/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 143.8/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 145.2/192.3 MB 32.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 146.9/192.3 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 148.3/192.3 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 149.8/192.3 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 151.1/192.3 MB 34.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 152.6/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 154.0/192.3 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 155.6/192.3 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 157.1/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 159.1/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 160.7/192.3 MB 32.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 162.1/192.3 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 164.1/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 165.5/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 167.0/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 169.5/192.3 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 170.8/192.3 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 172.2/192.3 MB 34.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 173.8/192.3 MB 34.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 175.3/192.3 MB 34.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 176.2/192.3 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 177.0/192.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 178.7/192.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 179.8/192.3 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 181.2/192.3 MB 31.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 182.6/192.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 184.0/192.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 185.5/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 187.0/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  188.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  189.6/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  191.5/192.3 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 192.3/192.3 MB 9.9 MB/s eta 0:00:00\n",
      "Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "   ---------------------------------------- 0.0/805.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 805.2/805.2 kB 25.6 MB/s eta 0:00:00\n",
      "Installing collected packages: lightning-utilities, torch, torchmetrics, pytorch_lightning\n",
      "Successfully installed lightning-utilities-0.10.0 pytorch_lightning-2.1.2 torch-2.1.1 torchmetrics-1.2.0\n",
      "Collecting adamp\n",
      "  Downloading adamp-0.3.0.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: adamp\n",
      "  Building wheel for adamp (setup.py): started\n",
      "  Building wheel for adamp (setup.py): finished with status 'done'\n",
      "  Created wheel for adamp: filename=adamp-0.3.0-py3-none-any.whl size=6012 sha256=003e22f71e32443c5af42a1e6138551bb4a3ddca504fc940dc3e2df8feddb9b2\n",
      "  Stored in directory: c:\\users\\82107\\appdata\\local\\pip\\cache\\wheels\\33\\f9\\d6\\b2ed816e1f321f6dcf72a99c954223b1259477095f40434979\n",
      "Successfully built adamp\n",
      "Installing collected packages: adamp\n",
      "Successfully installed adamp-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch_lightning\n",
    "!pip install adamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Ver 2.1.1+cpu\n",
      "Using Lightning Ver 2.1.2\n",
      "Fix Seed: 1102\n",
      "Submission ID: Classifier_1029\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts\n",
    "from torch.optim import AdamW\n",
    "from adamp import AdamP\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"Classifier\")\n",
    "parser.add_argument('--pretrained_model', default='kykim/funnel-kor-base', type=str)\n",
    "parser.add_argument('--pretrained_tokenizer', default='', type=str)\n",
    "parser.add_argument('--batch_size', default=16, type=int)\n",
    "parser.add_argument('--lr', default=4e-5, type=float)\n",
    "parser.add_argument('--epochs', default=5, type=int)\n",
    "parser.add_argument('--max_length', default=312, type=int)\n",
    "parser.add_argument('--train_data_path', default='./train.csv', type=str)\n",
    "parser.add_argument('--val_data_path', default='', type=str)\n",
    "parser.add_argument('--optimizer', default='AdamW')\n",
    "parser.add_argument('--lr_scheduler', default='none')\n",
    "parser.add_argument('--device', default=torch.device('cuda'), type=int)\n",
    "parser.add_argument('--mixed_precision', default=16, type=int)\n",
    "parser.add_argument('--cpu_workers', default=os.cpu_count(), type=int)\n",
    "parser.add_argument('--seed', default=1102, type=int)\n",
    "parser.add_argument('--date', default=1029, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "def set_seeds(seed=args.seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "set_seeds()\n",
    "os.chdir(\"C:/Users/82107/Documents/GitHub/hello\")\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "submission_id = f\"{parser.description}_{args.date}\"\n",
    "\n",
    "print(\"Using PyTorch Ver\", torch.__version__)\n",
    "print(\"Using Lightning Ver\", pl.__version__)\n",
    "print(\"Fix Seed:\", args.seed)\n",
    "print(\"Submission ID:\", submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "84   ì•„ì¹¨ì¼ì° ê°€ì„œ ê·¸ëŸ°ì§€ í’ˆì ˆì´ ìžì£¼ ë˜ëŠ” ì†Œê¸ˆë¹µì€ ìžˆì—ˆìœ¼ë‚˜ ìƒŒë“œìœ„ì¹˜ ë“±ì€ ë¶€ì¡±í–ˆìŒ. ...      1\n",
      "243  \"ì„±ì‹±ë‹´ ë¹µì§‘ì˜ ìœ„ì¹˜ê°€ ì¢‹ì•„ì„œ ìžì£¼ ë°©ë¬¸í•˜ê²Œ ë˜ëŠ”ë°, ì£¼ë³€ í™˜ê²½ë„ ì¡°ìš©í•˜ê³  ì•„ëŠ‘í•©ë‹ˆ...      0\n",
      "92   ï¿½ï¿½ðŸ¥ðŸ¥– ë„ˆë¬´ë‚˜ë„ ë‹¤ì–‘í•˜ê³  ì˜ˆìœ ë¹µì´ ë§Žì•„ì„œ ëˆˆì´ ëŒì•„ê°€ëŠ” ì¤„ ì•Œì•˜ì–´ìš”!!ðŸ¥ª ì´ë²ˆì—...      1\n",
      "195  \"ì„±ì‹±ë‹´ì—ì„œ êµ¬ìš´ ì‹ ì„ í•œ ë¹µì€ í•­ìƒ ê¸°ëŒ€ ì´ìƒì´ì—ìš”. ë°¤ ë¹µì€ ë°¤ì˜ ê³ ì†Œí•œ ë§›ê³¼ ë¶€...      0\n",
      "126  ëª…ë¬¼ì¸ íŠ€ê¹€ì†Œë³´ë¡œ ì¢…ë¥˜ ë‹¤ ë§›ìžˆê³ ìš”,,,,ìƒˆë¡­ê²Œ ë‚˜ì˜¨ ë¹µë“¤ë„ ë§›ìžˆì–´ìš”:) ë¬´ì—‡ë³´ë‹¤ ...      1                                                   text\n",
      "30   ë¹µ ì¢…ë¥˜ë„ ë§Žê³  ê°€ê²©ì´ íƒ€ ë©”ì´ì»¤ ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ì €ë ´í•©ë‹ˆë‹¤ê³„ì‚°ëŒ€ë„ ë§Žì•„ì„œ ì¤„ì´ ê¸ˆ...\n",
      "116                        ì¶œìž¥ê¸¸ì—ë“¤ë¦„íŠ€ê¹€ì†Œë³´ë¡œ ì´ˆì½”ì†Œë³´ë¡œ ë§›ìžˆê²Œ ë¨¹ì—ˆìŠµë‹ˆë‹¤\n",
      "79        ë¹µ ì²œêµ­. ì—¬ì „ížˆ íŠ€ê¹€ì†Œë³´ë¡œëŠ” ìµœê³ ì´ê³ ì• í”ŒíŒŒì´ ê³ ë¡œì¼€ ë°˜ë¯¸ ìš°ìœ ìƒí¬ë¦¼ ë‹¤ ë§›ìžˆìŒ\n",
      "127  ë§í•  ê²ƒë„ ì—†ì´ ìµœê³ ì˜ˆìš” ì§€í•˜ì² ì—­ ë°”ë¡œ ì•žì´ê³  ì†ë‹˜ ë§Žì€ë°ë„ ì§ì›ë¶„ë“¤ ì¹œì ˆí•˜ì‹œê³  ë¹µ...\n",
      "190  \"ëŒ€ì „ì˜ ì„±ì‹±ë‹´ ë¹µì§‘ì„ ë°©ë¬¸í•œ ê±´ ì •ë§ í–‰ìš´ì´ì—ˆì–´ìš”. ê° ë¹µì€ ê·¸ ìžì²´ë¡œ ì˜ˆìˆ ìž‘í’ˆ ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "data2 = pd.read_csv(\"C:/Users/82107/Documents/GitHub/train_ë§žì¶¤ë²•ê²€ì‚¬.csv\")\n",
    "traindata, testdata = train_test_split(data2, test_size=0.3, random_state=42)\n",
    "traindata = traindata.iloc[:,:2]\n",
    "testdata = testdata.iloc[:,:1]\n",
    "print(traindata.head(),testdata.head())\n",
    "train_df = traindata\n",
    "test_df = testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((188,), (188,), (81,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_df[\"text\"].values\n",
    "y = train_df[\"label\"].values\n",
    "X_test = test_df[\"text\"].values\n",
    "\n",
    "X.shape, y.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentence, label):\n",
    "        self.sentence = sentence\n",
    "        self.label = label\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentence[idx]\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded[\"input_ids\"][0]\n",
    "        token_type_ids = encoded[\"token_type_ids\"][0]\n",
    "        attention_masks = encoded[\"attention_mask\"][0]\n",
    "        \n",
    "        if self.label is not None:\n",
    "            label = self.label[idx]\n",
    "            return [input_ids, token_type_ids, attention_masks], label\n",
    "        else:\n",
    "            return [input_ids, token_type_ids, attention_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim= 384\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return F.silu(gate) * x\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, latent_dim=latent_dim):\n",
    "        super().__init__()\n",
    "        self.txt_model = AutoModel.from_pretrained(args.pretrained_model)\n",
    "        self.classifier = nn.Sequential(\n",
    "            SwiGLU(),\n",
    "            nn.Linear(latent_dim, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_ids = x[0]\n",
    "        token_type_ids = x[1]\n",
    "        attention_mask = x[2]\n",
    "        \n",
    "        txt_side = self.txt_model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        txt_feature = txt_side.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        output = self.classifier(txt_feature)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(LightningModule):\n",
    "    def __init__(self, backbone, args):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        return self.backbone(**kwargs)\n",
    "\n",
    "    def step(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.backbone(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(y_hat.squeeze(), y.float())\n",
    "        return loss, y, y_hat\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        pred = (y_hat > 0).float()\n",
    "        accuracy = (pred.squeeze() == y).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        pred = (y_hat > 0).float()\n",
    "        accuracy = (pred.squeeze() == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        pred = (y_hat > 0).float()\n",
    "        accuracy = (pred.squeeze() == y).float().mean()\n",
    "        self.log(\"test_acc:\", accuracy)\n",
    "\n",
    "    def predict_step(self, batch, dataloader_idx=0):\n",
    "        y_hat = self.backbone(batch)\n",
    "        return y_hat\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if args.optimizer == 'AdamW':\n",
    "            optimizer = AdamW(self.parameters(), lr=args.lr)\n",
    "        if args.optimizer == 'AdamP':\n",
    "            optimizer = AdamP(self.parameters(), lr=args.lr)\n",
    "        if args.lr_scheduler == \"none\":\n",
    "            return [optimizer]\n",
    "        \n",
    "        if args.lr_scheduler == 'cos':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
    "        if args.lr_scheduler == 'exp':\n",
    "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "        \n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82107\\anaconda3\\Lib\\site-packages\\lightning_fabric\\connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "c:\\Users\\82107\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:557: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\82107\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name     | Type     | Params\n",
      "--------------------------------------\n",
      "0 | backbone | Backbone | 185 M \n",
      "--------------------------------------\n",
      "185 M     Trainable params\n",
      "0         Non-trainable params\n",
      "185 M     Total params\n",
      "743.493   Total estimated model params size (MB)\n",
      "c:\\Users\\82107\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    }
   ],
   "source": [
    "val_acc_list = []\n",
    "preds_list = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "\n",
    "    X_train = X[train_index]\n",
    "    X_val = X[val_index]\n",
    "\n",
    "    y_train = y[train_index]\n",
    "    y_val = y[val_index]\n",
    "    \n",
    "    train_ds = CustomDataset(X_train, y_train)\n",
    "    val_ds = CustomDataset(X_val, y_val)\n",
    "    test_ds = CustomDataset(X_test, None)\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=args.cpu_workers)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.cpu_workers)\n",
    "    test_dataloader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.cpu_workers)\n",
    "\n",
    "    #\n",
    "    model = Model(Backbone(), args)\n",
    "\n",
    "    callbacks = [\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=\"saved/\", filename=f\"{args.pretrained_model}_{i}\",\n",
    "            monitor=\"val_acc\", mode=\"max\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        num_sanity_val_steps=0,\n",
    "        max_epochs=args.epochs, accelerator=\"auto\", callbacks=callbacks,\n",
    "        precision=args.mixed_precision,\n",
    "        devices=1\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    \n",
    "    #\n",
    "    ckpt = torch.load(f\"saved/{args.pretrained_model}_{i}.ckpt\")\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "    eval_dict = trainer.validate(model, dataloaders=val_dataloader)[0]\n",
    "    val_acc_list.append(eval_dict[\"val_acc\"])\n",
    "    \n",
    "    y_preds = trainer.predict(model, dataloaders=test_dataloader)\n",
    "    preds_list.append(np.vstack(y_preds))\n",
    "    \n",
    "val_acc_mean = np.mean(val_acc_list)\n",
    "\n",
    "print(f\"VAL FOLD MEAN: {val_acc_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
