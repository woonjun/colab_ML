{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\82107\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\82107\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\82107\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting pytorch_lightning\n",
      "  Obtaining dependency information for pytorch_lightning from https://files.pythonhosted.org/packages/60/eb/f29a4511a2675c9c14ca31cde4562f7676cf70396cf9e599210dca2f1e66/pytorch_lightning-2.1.2-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (1.24.3)\n",
      "Collecting torch>=1.12.0 (from pytorch_lightning)\n",
      "  Obtaining dependency information for torch>=1.12.0 from https://files.pythonhosted.org/packages/d6/a8/43e5033f9b2f727c158456e0720f870030ad3685c46f41ca3ca901b54922/torch-2.1.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torch-2.1.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (2023.4.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
      "  Obtaining dependency information for torchmetrics>=0.7.0 from https://files.pythonhosted.org/packages/a3/88/cc27059747ddecff744826e38014822023cbfff4ca079a6ee9a96602dd0b/torchmetrics-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from pytorch_lightning) (4.7.1)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
      "  Obtaining dependency information for lightning-utilities>=0.8.0 from https://files.pythonhosted.org/packages/5e/f4/07b748cb9834848de16aaeb1ae38bc9cfcfe3adc22ee2c8ebbe11db82795/lightning_utilities-0.10.0-py3-none-any.whl.metadata\n",
      "  Downloading lightning_utilities-0.10.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\82107\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\82107\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (68.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\82107\\anaconda3\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\82107\\anaconda3\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\82107\\anaconda3\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\82107\\anaconda3\\lib\\site-packages (from tqdm>=4.57.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\82107\\anaconda3\\lib\\site-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n",
      "Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
      "   ---------------------------------------- 0.0/776.9 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 409.6/776.9 kB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  768.0/776.9 kB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 776.9/776.9 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
      "Downloading torch-2.1.1-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/192.3 MB 17.2 MB/s eta 0:00:12\n",
      "   ---------------------------------------- 1.2/192.3 MB 19.3 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 1.5/192.3 MB 10.7 MB/s eta 0:00:18\n",
      "    --------------------------------------- 2.5/192.3 MB 13.2 MB/s eta 0:00:15\n",
      "    --------------------------------------- 3.6/192.3 MB 15.2 MB/s eta 0:00:13\n",
      "    --------------------------------------- 4.4/192.3 MB 15.7 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.2/192.3 MB 16.0 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.2/192.3 MB 16.0 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.8/192.3 MB 12.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 7.1/192.3 MB 14.2 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 8.5/192.3 MB 16.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 10.2/192.3 MB 17.2 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 11.6/192.3 MB 19.9 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 12.9/192.3 MB 22.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 14.4/192.3 MB 24.3 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 15.6/192.3 MB 32.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 17.0/192.3 MB 32.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 17.8/192.3 MB 32.7 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 19.0/192.3 MB 31.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 20.3/192.3 MB 29.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 21.5/192.3 MB 31.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 22.9/192.3 MB 29.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 24.2/192.3 MB 29.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 26.5/192.3 MB 29.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 28.1/192.3 MB 32.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 29.4/192.3 MB 32.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 30.8/192.3 MB 34.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 32.1/192.3 MB 32.7 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 32.3/192.3 MB 29.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 32.9/192.3 MB 28.4 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 33.2/192.3 MB 26.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 34.4/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 36.0/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 37.3/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 39.0/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 40.5/192.3 MB 26.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 42.2/192.3 MB 25.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 43.4/192.3 MB 31.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 45.0/192.3 MB 32.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 46.6/192.3 MB 32.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 48.5/192.3 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 49.9/192.3 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 51.9/192.3 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 53.2/192.3 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 54.7/192.3 MB 36.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 55.8/192.3 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 57.4/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 58.6/192.3 MB 32.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 60.1/192.3 MB 32.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 61.2/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 62.6/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 64.2/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 65.8/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 67.1/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 68.6/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 69.9/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 71.4/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 72.9/192.3 MB 36.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 74.3/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 75.5/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 77.1/192.3 MB 36.4 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 78.5/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 80.4/192.3 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 82.4/192.3 MB 34.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 84.4/192.3 MB 36.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 86.3/192.3 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 88.2/192.3 MB 38.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 89.7/192.3 MB 38.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 90.2/192.3 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 91.1/192.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 92.5/192.3 MB 31.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 94.6/192.3 MB 32.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 96.3/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 97.8/192.3 MB 31.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 99.4/192.3 MB 31.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 101.2/192.3 MB 38.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 102.8/192.3 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 104.5/192.3 MB 38.5 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 105.9/192.3 MB 36.3 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 107.5/192.3 MB 34.4 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.3/192.3 MB 16.4 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 110.9/192.3 MB 16.4 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 113.2/192.3 MB 17.2 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 115.4/192.3 MB 16.8 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 117.1/192.3 MB 16.8 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 119.1/192.3 MB 17.7 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 120.8/192.3 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 122.7/192.3 MB 43.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 124.0/192.3 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 125.3/192.3 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 127.2/192.3 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 128.2/192.3 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 129.4/192.3 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 131.2/192.3 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 132.2/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 133.6/192.3 MB 32.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 134.5/192.3 MB 29.8 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 136.1/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 137.9/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 139.1/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 140.3/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 142.0/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 143.8/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 145.2/192.3 MB 32.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 146.9/192.3 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 148.3/192.3 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 149.8/192.3 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 151.1/192.3 MB 34.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 152.6/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 154.0/192.3 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 155.6/192.3 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 157.1/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 159.1/192.3 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 160.7/192.3 MB 32.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 162.1/192.3 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 164.1/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 165.5/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 167.0/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 169.5/192.3 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 170.8/192.3 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 172.2/192.3 MB 34.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 173.8/192.3 MB 34.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 175.3/192.3 MB 34.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 176.2/192.3 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 177.0/192.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 178.7/192.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 179.8/192.3 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 181.2/192.3 MB 31.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 182.6/192.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 184.0/192.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 185.5/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 187.0/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  188.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  189.6/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  191.5/192.3 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 192.3/192.3 MB 9.9 MB/s eta 0:00:00\n",
      "Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "   ---------------------------------------- 0.0/805.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 805.2/805.2 kB 25.6 MB/s eta 0:00:00\n",
      "Installing collected packages: lightning-utilities, torch, torchmetrics, pytorch_lightning\n",
      "Successfully installed lightning-utilities-0.10.0 pytorch_lightning-2.1.2 torch-2.1.1 torchmetrics-1.2.0\n",
      "Collecting adamp\n",
      "  Downloading adamp-0.3.0.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: adamp\n",
      "  Building wheel for adamp (setup.py): started\n",
      "  Building wheel for adamp (setup.py): finished with status 'done'\n",
      "  Created wheel for adamp: filename=adamp-0.3.0-py3-none-any.whl size=6012 sha256=003e22f71e32443c5af42a1e6138551bb4a3ddca504fc940dc3e2df8feddb9b2\n",
      "  Stored in directory: c:\\users\\82107\\appdata\\local\\pip\\cache\\wheels\\33\\f9\\d6\\b2ed816e1f321f6dcf72a99c954223b1259477095f40434979\n",
      "Successfully built adamp\n",
      "Installing collected packages: adamp\n",
      "Successfully installed adamp-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch_lightning\n",
    "!pip install adamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Ver 2.1.1+cpu\n",
      "Using Lightning Ver 2.1.2\n",
      "Fix Seed: 1102\n",
      "Submission ID: Classifier_1029\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts\n",
    "from torch.optim import AdamW\n",
    "from adamp import AdamP\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"Classifier\")\n",
    "parser.add_argument('--pretrained_model', default='kykim/funnel-kor-base', type=str)\n",
    "parser.add_argument('--pretrained_tokenizer', default='', type=str)\n",
    "parser.add_argument('--batch_size', default=16, type=int)\n",
    "parser.add_argument('--lr', default=4e-5, type=float)\n",
    "parser.add_argument('--epochs', default=5, type=int)\n",
    "parser.add_argument('--max_length', default=312, type=int)\n",
    "parser.add_argument('--train_data_path', default='./train.csv', type=str)\n",
    "parser.add_argument('--val_data_path', default='', type=str)\n",
    "parser.add_argument('--optimizer', default='AdamW')\n",
    "parser.add_argument('--lr_scheduler', default='none')\n",
    "parser.add_argument('--device', default=torch.device('cuda'), type=int)\n",
    "parser.add_argument('--mixed_precision', default=16, type=int)\n",
    "parser.add_argument('--cpu_workers', default=os.cpu_count(), type=int)\n",
    "parser.add_argument('--seed', default=1102, type=int)\n",
    "parser.add_argument('--date', default=1029, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "def set_seeds(seed=args.seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "set_seeds()\n",
    "os.chdir(\"C:/Users/82107/Documents/GitHub/hello\")\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "submission_id = f\"{parser.description}_{args.date}\"\n",
    "\n",
    "print(\"Using PyTorch Ver\", torch.__version__)\n",
    "print(\"Using Lightning Ver\", pl.__version__)\n",
    "print(\"Fix Seed:\", args.seed)\n",
    "print(\"Submission ID:\", submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "84   아침일찍 가서 그런지 품절이 자주 되는 소금빵은 있었으나 샌드위치 등은 부족했음. ...      1\n",
      "243  \"성싱담 빵집의 위치가 좋아서 자주 방문하게 되는데, 주변 환경도 조용하고 아늑합니...      0\n",
      "92   ��🥐🥖 너무나도 다양하고 예쁜 빵이 많아서 눈이 돌아가는 줄 알았어요!!🥪 이번에...      1\n",
      "195  \"성싱담에서 구운 신선한 빵은 항상 기대 이상이에요. 밤 빵은 밤의 고소한 맛과 부...      0\n",
      "126  명물인 튀김소보로 종류 다 맛있고요,,,,새롭게 나온 빵들도 맛있어요:) 무엇보다 ...      1                                                   text\n",
      "30   빵 종류도 많고 가격이 타 메이커 보다 상대적으로 저렴합니다계산대도 많아서 줄이 금...\n",
      "116                        출장길에들름튀김소보로 초코소보로 맛있게 먹었습니다\n",
      "79        빵 천국. 여전히 튀김소보로는 최고이고애플파이 고로케 반미 우유생크림 다 맛있음\n",
      "127  말할 것도 없이 최고예요 지하철역 바로 앞이고 손님 많은데도 직원분들 친절하시고 빵...\n",
      "190  \"대전의 성싱담 빵집을 방문한 건 정말 행운이었어요. 각 빵은 그 자체로 예술작품 ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "data2 = pd.read_csv(\"C:/Users/82107/Documents/GitHub/train_맞춤법검사.csv\")\n",
    "traindata, testdata = train_test_split(data2, test_size=0.3, random_state=42)\n",
    "traindata = traindata.iloc[:,:2]\n",
    "testdata = testdata.iloc[:,:1]\n",
    "print(traindata.head(),testdata.head())\n",
    "train_df = traindata\n",
    "test_df = testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((188,), (188,), (81,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_df[\"text\"].values\n",
    "y = train_df[\"label\"].values\n",
    "X_test = test_df[\"text\"].values\n",
    "\n",
    "X.shape, y.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentence, label):\n",
    "        self.sentence = sentence\n",
    "        self.label = label\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentence[idx]\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded[\"input_ids\"][0]\n",
    "        token_type_ids = encoded[\"token_type_ids\"][0]\n",
    "        attention_masks = encoded[\"attention_mask\"][0]\n",
    "        \n",
    "        if self.label is not None:\n",
    "            label = self.label[idx]\n",
    "            return [input_ids, token_type_ids, attention_masks], label\n",
    "        else:\n",
    "            return [input_ids, token_type_ids, attention_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim= 384\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return F.silu(gate) * x\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, latent_dim=latent_dim):\n",
    "        super().__init__()\n",
    "        self.txt_model = AutoModel.from_pretrained(args.pretrained_model)\n",
    "        self.classifier = nn.Sequential(\n",
    "            SwiGLU(),\n",
    "            nn.Linear(latent_dim, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_ids = x[0]\n",
    "        token_type_ids = x[1]\n",
    "        attention_mask = x[2]\n",
    "        \n",
    "        txt_side = self.txt_model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        txt_feature = txt_side.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        output = self.classifier(txt_feature)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(LightningModule):\n",
    "    def __init__(self, backbone, args):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        return self.backbone(**kwargs)\n",
    "\n",
    "    def step(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.backbone(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(y_hat.squeeze(), y.float())\n",
    "        return loss, y, y_hat\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        pred = (y_hat > 0).float()\n",
    "        accuracy = (pred.squeeze() == y).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        pred = (y_hat > 0).float()\n",
    "        accuracy = (pred.squeeze() == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        pred = (y_hat > 0).float()\n",
    "        accuracy = (pred.squeeze() == y).float().mean()\n",
    "        self.log(\"test_acc:\", accuracy)\n",
    "\n",
    "    def predict_step(self, batch, dataloader_idx=0):\n",
    "        y_hat = self.backbone(batch)\n",
    "        return y_hat\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if args.optimizer == 'AdamW':\n",
    "            optimizer = AdamW(self.parameters(), lr=args.lr)\n",
    "        if args.optimizer == 'AdamP':\n",
    "            optimizer = AdamP(self.parameters(), lr=args.lr)\n",
    "        if args.lr_scheduler == \"none\":\n",
    "            return [optimizer]\n",
    "        \n",
    "        if args.lr_scheduler == 'cos':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
    "        if args.lr_scheduler == 'exp':\n",
    "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "        \n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82107\\anaconda3\\Lib\\site-packages\\lightning_fabric\\connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "c:\\Users\\82107\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:557: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\82107\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name     | Type     | Params\n",
      "--------------------------------------\n",
      "0 | backbone | Backbone | 185 M \n",
      "--------------------------------------\n",
      "185 M     Trainable params\n",
      "0         Non-trainable params\n",
      "185 M     Total params\n",
      "743.493   Total estimated model params size (MB)\n",
      "c:\\Users\\82107\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    }
   ],
   "source": [
    "val_acc_list = []\n",
    "preds_list = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "\n",
    "    X_train = X[train_index]\n",
    "    X_val = X[val_index]\n",
    "\n",
    "    y_train = y[train_index]\n",
    "    y_val = y[val_index]\n",
    "    \n",
    "    train_ds = CustomDataset(X_train, y_train)\n",
    "    val_ds = CustomDataset(X_val, y_val)\n",
    "    test_ds = CustomDataset(X_test, None)\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=args.cpu_workers)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.cpu_workers)\n",
    "    test_dataloader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.cpu_workers)\n",
    "\n",
    "    #\n",
    "    model = Model(Backbone(), args)\n",
    "\n",
    "    callbacks = [\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=\"saved/\", filename=f\"{args.pretrained_model}_{i}\",\n",
    "            monitor=\"val_acc\", mode=\"max\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        num_sanity_val_steps=0,\n",
    "        max_epochs=args.epochs, accelerator=\"auto\", callbacks=callbacks,\n",
    "        precision=args.mixed_precision,\n",
    "        devices=1\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    \n",
    "    #\n",
    "    ckpt = torch.load(f\"saved/{args.pretrained_model}_{i}.ckpt\")\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "    eval_dict = trainer.validate(model, dataloaders=val_dataloader)[0]\n",
    "    val_acc_list.append(eval_dict[\"val_acc\"])\n",
    "    \n",
    "    y_preds = trainer.predict(model, dataloaders=test_dataloader)\n",
    "    preds_list.append(np.vstack(y_preds))\n",
    "    \n",
    "val_acc_mean = np.mean(val_acc_list)\n",
    "\n",
    "print(f\"VAL FOLD MEAN: {val_acc_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
